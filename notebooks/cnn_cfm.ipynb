{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from jax import jit\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from cone.utils.misc import pshape\n",
    "from einops import rearrange\n",
    "from cone.utils.plot import scatter_movie, imshow_movie\n",
    "from cone.utils.misc import meanvmap, tracewrap\n",
    "from jax import jacrev, jacfwd, vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the EMNIST dataset with (image, label) pairs\n",
    "ds_train = tfds.load(\"emnist/balanced\", split=\"train\", as_supervised=True)\n",
    "ds_test = tfds.load(\"emnist/balanced\", split=\"test\", as_supervised=True)\n",
    "\n",
    "\n",
    "def preprocess_emnist(image, label):\n",
    "    image = tf.image.rot90(image, k=3)\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "# Preprocess the training dataset\n",
    "ds_train = ds_train.map(preprocess_emnist, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.shuffle(10000)\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Preprocess the test dataset\n",
    "ds_test = ds_test.map(preprocess_emnist, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Convert datasets to NumPy iterators for JAX\n",
    "ds_train = tfds.as_numpy(ds_train)\n",
    "ds_test = tfds.as_numpy(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_fn(vae, beta):\n",
    "    def loss_fn(params, x, key):\n",
    "\n",
    "        logits, mu, logvar = vae.apply(params, x, rngs={\"latent\": key})\n",
    "\n",
    "        recon_loss = (\n",
    "            optax.sigmoid_binary_cross_entropy(logits=logits, labels=x)\n",
    "            .sum(axis=[1, 2, 3])\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        kl_loss = -0.5 * jnp.mean(\n",
    "            jnp.sum(1 + logvar - (mu) ** 2 - jnp.exp(logvar), axis=-1)\n",
    "        )\n",
    "\n",
    "        # vars = jnp.ones_like(y)\n",
    "\n",
    "        # kl_loss = kl_divergence_v(biases, vars, mu, logvar)\n",
    "\n",
    "        total_loss = recon_loss + beta * kl_loss\n",
    "        return total_loss, (recon_loss, kl_loss)\n",
    "\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cone.net.vae import VAE\n",
    "from cone.utils.misc import count_params\n",
    "\n",
    "\n",
    "latent_features = 32\n",
    "encoder_features = [1, 64, 48, 32]\n",
    "decoder_features = [32, 48, 64, 1]\n",
    "kernel = 2\n",
    "key = jax.random.PRNGKey(1)\n",
    "vae = VAE(\n",
    "    latent_features=latent_features,\n",
    "    encoder_features=encoder_features,\n",
    "    decoder_features=decoder_features,\n",
    "    kernel_size=kernel,\n",
    "    padding=\"SAME\",\n",
    ")\n",
    "\n",
    "\n",
    "x_dummy = jnp.zeros((batch_size, 28, 28, 1))\n",
    "key = jax.random.PRNGKey(0)\n",
    "p_key, l_key, key = jax.random.split(key, num=3)\n",
    "init_rng = {\"params\": p_key, \"latent\": l_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samplers(vae, params, x):\n",
    "    _, _, shape = vae.apply(params, x, method=vae.encode)\n",
    "\n",
    "    @jit\n",
    "    def encode(params, x, key):\n",
    "        z = vae.apply(params, x, key, method=vae.encode_latent)\n",
    "        return z\n",
    "\n",
    "    @jit\n",
    "    def decode(params, x):\n",
    "        logits = vae.apply(params, x, shape, method=vae.decode)\n",
    "        return nn.sigmoid(logits)\n",
    "\n",
    "    return encode, decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_init = vae.init(init_rng, x_dummy)\n",
    "encoder, decoder = get_samplers(vae, params_init, x_dummy)\n",
    "print(count_params(params_init))\n",
    "\n",
    "n_epochs = 25\n",
    "# set up the optimizer\n",
    "n_batches = len(ds_train)\n",
    "n_steps = n_epochs * n_batches\n",
    "learning_rate = optax.cosine_decay_schedule(1e-3, decay_steps=n_epochs * n_steps)\n",
    "optimizer = optax.adam(learning_rate)\n",
    "optimizer_state = optimizer.init(params_init)\n",
    "\n",
    "\n",
    "beta = 0.01\n",
    "loss_fn = get_loss_fn(vae, beta)\n",
    "\n",
    "\n",
    "@jit\n",
    "def train_step(params, x, optimizer_state, key):\n",
    "    skey, key = jax.random.split(key)\n",
    "    (loss, aux), grads = jax.value_and_grad(loss_fn, has_aux=True)(params, x, skey)\n",
    "    updates, optimizer_state = optimizer.update(grads, optimizer_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, optimizer_state, loss, aux, key\n",
    "\n",
    "\n",
    "opt_params = params_init\n",
    "# training loop\n",
    "pbar = tqdm(total=n_batches)\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    pbar.reset(total=n_batches)\n",
    "    pbar.set_description(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "\n",
    "    for batch in ds_train:\n",
    "\n",
    "        x, y = batch\n",
    "\n",
    "        opt_params, optimizer_state, loss, aux, key = train_step(\n",
    "            opt_params, x, optimizer_state, key\n",
    "        )\n",
    "        rc_loss, kl_loss = aux\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"loss\": f\"{loss:.2E}\",\n",
    "                \"rc_loss\": f\"{rc_loss:.2E}\",\n",
    "                \"kl_loss\": f\"{kl_loss:.2E}\",\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plt = 25\n",
    "n_grid = int(np.sqrt(n_plt))\n",
    "logits, mean, logvar = vae.apply(opt_params, x[:n_plt], rngs={\"latent\": key})\n",
    "logits = rearrange(logits, \"(n1 n2) h w c -> (n1 h) (n2 w) c\", n1=n_grid, n2=n_grid)\n",
    "recon = nn.sigmoid(logits)\n",
    "plt.imshow(recon)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randn(n_plt, latent_features)\n",
    "samples = decoder(opt_params, z)\n",
    "samples = rearrange(samples, \"(n1 n2) h w c -> (n1 h) (n2 w) c\", n1=n_grid, n2=n_grid)\n",
    "plt.imshow(samples)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = []\n",
    "labels = []\n",
    "n_test_batches = 512\n",
    "for i, (x, y) in enumerate(ds_train):\n",
    "    key, skey = jax.random.split(key)\n",
    "    z = encoder(opt_params, x, skey)\n",
    "    latents.append(z)\n",
    "    labels.append(y)\n",
    "    if i > n_test_batches:\n",
    "        break\n",
    "\n",
    "latents = np.concatenate(latents)\n",
    "labels = np.concatenate(labels)\n",
    "latents.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_latents = []\n",
    "classes = np.sort(np.unique(labels))\n",
    "min_amt = np.inf\n",
    "for c in classes:\n",
    "    d = latents[labels == c]\n",
    "    t_latents.append(d)\n",
    "    min_amt = min(min_amt, len(d))\n",
    "\n",
    "t_latents = np.asarray([tl[:min_amt] for tl in t_latents])\n",
    "t_eval = jnp.asarray(classes) / classes.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cone.utils.misc import get_rand_idx, interplate_in_t\n",
    "from cone.integrate.quad import get_simpson_quadrature, get_gauss_quadrature\n",
    "\n",
    "\n",
    "def get_sample_fn(\n",
    "    X_data, t_data, bs_tau=16, bs_t=256, bs_n=256, quad=\"simp\", jit_fn=True\n",
    "):\n",
    "\n",
    "    t_data = t_data / t_data[-1]  # normalize in [0,1]\n",
    "\n",
    "    # if we are doing monte carlo, we dont need to interpolate\n",
    "    if quad != \"mc\":\n",
    "        if quad == \"simp\":\n",
    "            # odd number of points necessary for simpsons\n",
    "            if (bs_t - 1) % 2 != 0:\n",
    "                bs_t += 1\n",
    "            t_batch, quad_weights = get_simpson_quadrature(bs_t)\n",
    "        elif quad == \"gauss\":\n",
    "            t_batch, quad_weights = get_gauss_quadrature(bs_t)\n",
    "\n",
    "        # add start and end points for boundary term\n",
    "        start, end = jnp.asarray([0]), jnp.asarray([1.0])\n",
    "        t_batch = jnp.concatenate([start, t_batch, end])\n",
    "        X_batch = interplate_in_t(X_data, t_data, t_batch)\n",
    "\n",
    "        X_batch = jnp.asarray(X_batch)\n",
    "        t_batch = jnp.asarray(t_batch)\n",
    "    else:\n",
    "\n",
    "        X_batch = jnp.asarray(X_data)\n",
    "        t_batch = jnp.asarray(t_data)\n",
    "        quad_weights = jnp.ones(bs_t) / bs_t\n",
    "\n",
    "    t_batch = t_batch.reshape(-1, 1)\n",
    "\n",
    "    def sample_fn(in_key):\n",
    "\n",
    "        nonlocal X_batch\n",
    "        nonlocal t_batch\n",
    "\n",
    "        T, N, D = X_batch.shape\n",
    "        T, one = t_batch.shape\n",
    "\n",
    "        in_key, tau_key = jax.random.split(in_key)\n",
    "\n",
    "        tau_batch = jax.random.uniform(tau_key, (bs_tau, 1))\n",
    "        tau_batch = jnp.sort(tau_batch)\n",
    "        bs_t_a = min(bs_t, T - 1)\n",
    "\n",
    "        if quad == \"mc\":\n",
    "            in_key, key_t = jax.random.split(in_key)\n",
    "            t_idx = jax.random.choice(key_t, T - 1, shape=(bs_t_a,), replace=False)\n",
    "            t_idx = jnp.sort(t_idx)\n",
    "            t_batch = t_batch[t_idx]\n",
    "            X_batch = X_batch[t_idx]\n",
    "\n",
    "        # keys = jax.random.split(in_key, num=X_batch.shape[0])\n",
    "        # sample_idx = vmap(get_rand_idx, (0, None, None))(keys, N, bs_n)\n",
    "        # rows = jnp.arange(X_batch.shape[0])[:, jnp.newaxis]\n",
    "        # X_batch = X_batch[rows, sample_idx]\n",
    "\n",
    "        in_key, x_key = jax.random.split(in_key)\n",
    "        sample_idx = get_rand_idx(x_key, N, bs_n)\n",
    "        X_batch = X_batch[:, sample_idx]\n",
    "\n",
    "        return tau_batch, X_batch, t_batch\n",
    "\n",
    "    if jit_fn:\n",
    "        sample_fn = jit(sample_fn)\n",
    "\n",
    "    return sample_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cone.utils.misc import meanvmap, tracewrap, sqwrap, key_tensor, fold_in_data\n",
    "from jax import jacrev, jacfwd, grad\n",
    "\n",
    "\n",
    "def get_flow_loss_fn(s_fn):\n",
    "\n",
    "    @sqwrap\n",
    "    def alpha(tau):\n",
    "        t_fn = lambda tau: jnp.cos(jnp.pi * tau) ** 2\n",
    "        f_fn = lambda tau: tau * 0.0\n",
    "        res = jax.lax.cond(tau >= 0.5, t_fn, f_fn, tau)\n",
    "        return res\n",
    "\n",
    "    @sqwrap\n",
    "    def beta(tau):\n",
    "        t_fn = lambda tau: jnp.cos(jnp.pi * tau) ** 2\n",
    "        f_fn = lambda tau: tau * 0.0\n",
    "        res = jax.lax.cond(tau < 0.5, t_fn, f_fn, tau)\n",
    "        return res\n",
    "\n",
    "    @sqwrap\n",
    "    def gamma(tau):\n",
    "        return jnp.sin(jnp.pi * tau) ** 2\n",
    "        #return jnp.sqrt(2 * tau * (1 - tau))\n",
    "    # @sqwrap\n",
    "    # def alpha(tau):\n",
    "    #     return tau\n",
    "\n",
    "    # @sqwrap\n",
    "    # def beta(tau):\n",
    "    #     return 1-tau\n",
    "\n",
    "    def interpolant(tau, xt, xt_m1):\n",
    "        return alpha(tau) * xt + beta(tau) * xt_m1\n",
    "\n",
    "    @sqwrap\n",
    "    def interpolant(tau, xt_p1, xt):\n",
    "        return alpha(tau) * xt_p1 + beta(tau) * xt\n",
    "\n",
    "    interpolant_dt = jacrev(interpolant)\n",
    "    gamma_dt = grad(gamma)\n",
    "\n",
    "    def flow_match(x_tp1, xt, t, tau, params, key):\n",
    "\n",
    "        r_data = fold_in_data(x_tp1, xt, t, tau)\n",
    "        key = jax.random.fold_in(key, r_data)\n",
    "\n",
    "        tau = jnp.squeeze(tau)\n",
    "        t = jnp.squeeze(t)\n",
    "\n",
    "        D = x_tp1.shape[0]\n",
    "\n",
    "        noise_i, noise_l = jax.random.normal(key, shape=(2, D))\n",
    "\n",
    "        dt_i = interpolant_dt(tau, x_tp1, xt)\n",
    "\n",
    "        g_dt = gamma_dt(tau)\n",
    "        dt_i = jnp.squeeze(dt_i)\n",
    "\n",
    "        x_tau_plus = interpolant(tau, x_tp1, xt) + gamma(tau) * noise_i\n",
    "        s_plus = s_fn(tau.reshape(1), x_tau_plus, t.reshape(1), params)\n",
    "\n",
    "        x_tau_minus = interpolant(tau, x_tp1, xt) - gamma(tau) * noise_i\n",
    "        s_minus = s_fn(tau.reshape(1), x_tau_minus, t.reshape(1), params)\n",
    "\n",
    "        l_plus = jnp.dot(s_plus, s_plus) - 2 * jnp.dot(s_plus, dt_i + g_dt * noise_l)\n",
    "        l_minus = jnp.dot(s_minus, s_minus) - 2 * jnp.dot(\n",
    "            s_minus, dt_i + g_dt * noise_l\n",
    "        )\n",
    "\n",
    "        l = (l_plus + l_minus) / 2\n",
    "        return jnp.squeeze(l)\n",
    "\n",
    "    fm_Vx = meanvmap(flow_match, in_axes=(0, 0, None, None, None, None))\n",
    "    fm_Vx_Vtau = meanvmap(fm_Vx, in_axes=(None, None, None, 0, None, None))\n",
    "    fm_Vx_Vtau_Vt = meanvmap(fm_Vx_Vtau, in_axes=(0, 0, 0, None, None, None))\n",
    "\n",
    "    def loss_fn(params, tau_batch, X_tp1, X_t, t, key):\n",
    "\n",
    "\n",
    "        loss = fm_Vx_Vtau_Vt(X_tp1, X_t, t, tau_batch, params, key)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cone.utils.misc import get_rand_idx, interplate_in_t\n",
    "from cone.integrate.quad import get_simpson_quadrature, get_gauss_quadrature\n",
    "\n",
    "\n",
    "def get_sample_fn(X_data, t_data, bs_tau=16, bs_t=256, bs_n=256):\n",
    "\n",
    "    t_data = t_data / t_data[-1]  # normalize in [0,1]\n",
    "    X_data = jnp.asarray(X_data)\n",
    "    t_data = jnp.asarray(t_data).reshape(-1, 1)\n",
    "\n",
    "    def sample_fn(in_key):\n",
    "\n",
    "        nonlocal X_data\n",
    "        nonlocal t_data\n",
    "\n",
    "        T, N, D = X_data.shape\n",
    "        T, one = t_data.shape\n",
    "\n",
    "        key_t, x_key, tau_key = jax.random.split(in_key, num=3)\n",
    "\n",
    "        # sample tau\n",
    "        tau_batch = jax.random.uniform(tau_key, (bs_tau, 1), minval=0.0, maxval=1.0)\n",
    "        tau_batch = jnp.sort(tau_batch)\n",
    "\n",
    "        # sample x\n",
    "        sample_idx = get_rand_idx(x_key, N, bs_n)\n",
    "        X_batch = X_data[:, sample_idx]\n",
    "\n",
    "        # sample t\n",
    "        t_idx = jax.random.choice(key_t, 10, shape=(1,), replace=False)\n",
    "        t_idx = jnp.squeeze(t_idx).astype(jnp.int32)\n",
    "\n",
    "        \n",
    "        t = t_data[t_idx:t_idx+1]\n",
    "        X_t = X_batch[t_idx:t_idx+1]\n",
    "        X_tp1 = X_batch[t_idx+1:t_idx+2]\n",
    "\n",
    "        return tau_batch, X_tp1, X_t, t\n",
    "\n",
    "\n",
    "    return sample_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cone.net.mlp import MLP\n",
    "from cone.net.adam import adam_opt\n",
    "from cone.utils.misc import normalize_data\n",
    "\n",
    "features = [64, 64, 64, 64, 64, latent_features]\n",
    "s_net = MLP(features=features, squeeze=True)\n",
    "\n",
    "\n",
    "def s_fn(tau, x, t, params):\n",
    "    tau_x_t = jnp.concatenate([tau, x, t])\n",
    "    return s_net.apply(params, tau_x_t)\n",
    "\n",
    "\n",
    "x_dummy = jnp.ones((64, latent_features + 2))\n",
    "s_params_init = s_net.init(key, x_dummy)\n",
    "\n",
    "\n",
    "loss_fn = get_flow_loss_fn(s_fn)\n",
    "sample_fn = get_sample_fn(t_latents, t_eval, bs_tau=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "key = jax.random.PRNGKey(random.randint(0,1e6))\n",
    "tau_batch, X_t, X_tp1, t = sample_fn(key)\n",
    "jnp.squeeze(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=X_t[0, :, 0], y=X_t[0, :, 1])\n",
    "plt.scatter(x=X_tp1[0, :, 0], y=X_tp1[0, :, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cone.net.adam import adam_opt\n",
    "\n",
    "opt_params_s, loss_history = adam_opt(\n",
    "    s_params_init,\n",
    "    loss_fn,\n",
    "    sample_fn,\n",
    "    steps=5000,\n",
    "    learning_rate=5e-3,\n",
    "    verbose=True,\n",
    "    key=key,\n",
    "    loss_key=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cone.integrate.sde import odeint_rk4\n",
    "\n",
    "\n",
    "def solve_test_cfm(s_fn, params, ics, t_int, T):\n",
    "\n",
    "    s_Vx = vmap(s_fn, (None, 0, None, None))\n",
    "\n",
    "    @jit\n",
    "    def integrate(ics, physical_t, params):\n",
    "\n",
    "        def fn(tau, y):\n",
    "\n",
    "            return s_Vx(tau, y, physical_t, params)\n",
    "\n",
    "        sol = odeint_rk4(fn, ics, taus)\n",
    "\n",
    "        return sol\n",
    "\n",
    "    sols = []\n",
    "    taus = jnp.linspace(0, 1, T).reshape(-1, 1)\n",
    "    t_int = t_int.reshape(-1, 1)\n",
    "    for physical_t in tqdm(t_int, desc=\"CFM\"):\n",
    "        sol = integrate(ics, physical_t, params)\n",
    "        sols.append(sol)\n",
    "        ics = sol[-1]\n",
    "        \n",
    "        # test_sol.append(ics)\n",
    "\n",
    "    sols = jnp.concatenate(sols)\n",
    "    sols = jnp.squeeze(sols)\n",
    "\n",
    "    return sols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = t_latents[0]\n",
    "t_int = t_eval\n",
    "T_tau = 32\n",
    "n_plot = 1000\n",
    "idx_sample = np.linspace(0, ic.shape[0] - 1, n_plot, dtype=np.uint32)\n",
    "\n",
    "sol_cfm = solve_test_cfm(s_fn, opt_params_s, ic[idx_sample], t_int, T_tau)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
